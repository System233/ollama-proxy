/*
Copyright (c) 2025 System233

This source code is licensed under the MIT license found in the
LICENSE file in the root directory of this source tree.

This file is part of the ollama-proxy project.
*/

package main

import (
	"bytes"
	"encoding/json"
	"fmt"
	"io"
	"log"
	"net/http"
	"os"
	"strings"
	"time"

	"github.com/fsnotify/fsnotify"
	"github.com/gin-gonic/gin"
	"github.com/ollama/ollama/api"
	"github.com/ollama/ollama/types/model"
	"github.com/sashabaranov/go-openai"
	"gopkg.in/yaml.v3"
)

// =============================
// 配置结构
// =============================
type Config struct {
	Server struct {
		Listen  string `yaml:"listen"`
		Mode    string `yaml:"mode"`
		Version string `yaml:"version"`
	} `yaml:"server"`
	Endpoints []Endpoint `yaml:"endpoints"`
}

type Endpoint struct {
	ID         string `yaml:"id"`
	BaseURL    string `yaml:"base_url"`
	APIKey     string `yaml:"api_key"`
	APIKeyName string `yaml:"api_key_name"`
	ModelTrim  struct {
		Prefix string `yaml:"prefix"`
		Suffix string `yaml:"suffix"`
	} `yaml:"model_trim"`
	Headers map[string]string `yaml:"headers"`
}

func (ep *Endpoint) NewRequest(method, path string, req any) (*http.Response, error) {
	var body io.Reader
	if req != nil {
		data, _ := json.Marshal(req)
		body = bytes.NewBuffer(data)
	}
	url := fmt.Sprintf("%s%s", strings.TrimRight(ep.BaseURL, "/"), path)
	httpReq, err := http.NewRequest(method, url, body)
	if err != nil {
		return nil, err
	}
	if ep.APIKey != "" {
		name := ep.APIKeyName
		if name == "" {
			name = "Authorization"
		}
		httpReq.Header.Set(name, "Bearer "+ep.APIKey)
	}
	httpReq.Header.Set("Content-Type", "application/json")
	for key, val := range ep.Headers {
		httpReq.Header.Set(key, val)
	}
	return http.DefaultClient.Do(httpReq)

}
func (ep *Endpoint) OpenAIModels() (*openai.ModelsList, error) {
	resp, err := ep.NewRequest("GET", "/models", nil)
	if err != nil {
		return nil, err
	}
	data, err := io.ReadAll(resp.Body)
	if err != nil {
		return nil, err
	}
	resp.Body.Close()
	var parsed openai.ModelsList
	err = json.Unmarshal(data, &parsed)
	if err != nil {
		return nil, err
	}
	for index, item := range parsed.Models {
		itemId := item.ID
		if ep.ModelTrim.Prefix != "" {
			itemId = strings.TrimPrefix(itemId, ep.ModelTrim.Prefix)
		}
		if ep.ModelTrim.Suffix != "" {
			itemId = strings.TrimSuffix(itemId, ep.ModelTrim.Suffix)
		}
		parsed.Models[index].ID = fmt.Sprintf("%s/%s", ep.ID, itemId)
	}
	return &parsed, nil
}

//	type Tag struct{
//		api.ListModelResponse
//		api.
//	}
func (ep *Endpoint) OllamaModels() ([]api.ListModelResponse, error) {
	parsed, err := ep.OpenAIModels()
	if err != nil {
		return nil, err
	}

	var allModels []api.ListModelResponse
	for _, m := range parsed.Models {
		allModels = append(allModels, api.ListModelResponse{
			Name:       m.ID,
			Model:      m.ID,
			ModifiedAt: time.Now(), // 可从本地文件获取实际时间
			Size:       0,
			Digest:     "n/a", // 可计算实际 hash
			Details: api.ModelDetails{
				ParentModel:       "",
				Format:            "gguf",
				Family:            "proxy",
				Families:          []string{"proxy"},
				ParameterSize:     "n/a",
				QuantizationLevel: "n/a",
			},
		})
	}
	return allModels, nil
}
func (ep *Endpoint) detail(modelId string) ([]byte, error) {
	path := fmt.Sprintf("/models/%s", modelId)
	resp, err := ep.NewRequest("GET", path, nil)
	if err != nil {
		return nil, err
	}
	defer resp.Body.Close()
	data, err := io.ReadAll(resp.Body)
	if err != nil {
		return nil, err
	}
	return data, nil
}

func (ep *Endpoint) OpenAIModel(modelId string) (*openai.Model, error) {
	data, err := ep.detail(modelId)
	if err != nil {
		return nil, err
	}
	var parsed openai.Model
	err = json.Unmarshal(data, &parsed)
	if err != nil {
		return nil, err
	}
	return &parsed, nil
}
func (ep *Endpoint) OllamaModel(modelId string) (*api.ShowResponse, error) {
	data, err := ep.detail(modelId)
	if err != nil {
		return nil, err
	}
	var modelInfo map[string]any
	err = json.Unmarshal(data, &modelInfo)
	if err != nil {
		return nil, err
	}
	result := api.ShowResponse{
		Modelfile:  fmt.Sprintf("# Modelfile generated by \"ollama show\"\n# FROM %s\n", modelId),
		Parameters: "num_ctx 4096",
		Template:   "{{ .Prompt }} -> {{ .Response }}",
		Details: api.ModelDetails{
			ParentModel:       "",
			Format:            "gguf",
			Family:            "proxy",
			Families:          []string{"proxy"},
			ParameterSize:     "n/a",
			QuantizationLevel: "n/a",
		},
		ModelInfo: modelInfo,
		Capabilities: []model.Capability{
			model.CapabilityCompletion,
			model.CapabilityTools,
		},
	}
	return &result, nil
}

var (
	config      Config
	endpointMap map[string]Endpoint
)

// =============================
// 加载配置文件
// =============================
func loadConfig(path string) error {
	data, err := os.ReadFile(path)
	if err != nil {
		log.Printf("读取配置文件失败: %v", err)
		return err
	}
	if err := yaml.Unmarshal(data, &config); err != nil {
		log.Printf("解析配置文件失败: %v", err)
		return err
	}

	endpointMap = make(map[string]Endpoint)
	for _, ep := range config.Endpoints {
		log.Println("注册端点:", ep.ID, "->", ep.BaseURL)
		endpointMap[ep.ID] = ep
	}
	return nil
}
func watchConfig(path string) {
	log.Printf("监控配置文件更改:%s", path)
	watcher, err := fsnotify.NewWatcher()
	if err != nil {
		log.Printf("NewWatcher:%v", err)
		return
	}
	watcher.Add(path)

	for {
		select {
		case event, ok := <-watcher.Events:
			if !ok {
				log.Printf("watcher channel closed")
				return
			}
			if event.Op&fsnotify.Write == fsnotify.Write {
				log.Printf("重载配置 %s", event.Name)
				if err := loadConfig(path); err != nil {
					log.Printf("loadConfig:%v", err)
				}
			}
		case err, ok := <-watcher.Errors:
			if !ok {
				log.Printf("watcher error:%v", err)
				return
			}
		}
	}
}

// ========== 选择端点与剥离前缀 ==========
func pickEndpoint(prefixed string) (Endpoint, string, bool) {
	prefixed = strings.TrimSpace(prefixed)
	for id, ep := range endpointMap {
		if res, ok := strings.CutPrefix(prefixed, id+"/"); ok {
			return ep, res, true
		}
	}
	return Endpoint{}, "", false
}

// =============================
// 版本信息
// =============================
type VersionInfo struct {
	Version string `json:"version"`
}

func getVersion(c *gin.Context) {
	c.JSON(http.StatusOK, VersionInfo{
		Version: config.Server.Version,
	})
}

func getOllamaTags(c *gin.Context) {
	var allModels []api.ListModelResponse
	for _, ep := range config.Endpoints {
		models, err := ep.OllamaModels()
		if err != nil {
			log.Printf("获取端点[%s]模型失败: %v", ep.ID, err)
			continue
		}
		allModels = append(allModels, models...)
	}

	c.JSON(http.StatusOK, api.ListResponse{Models: allModels})
}

func postOllamaShow(c *gin.Context) {
	var req api.ShowRequest
	if err := c.ShouldBindJSON(&req); err != nil {
		c.JSON(http.StatusBadRequest, gin.H{"error": "解析请求体失败", "detail": err.Error()})
		return
	}

	ep, actualModel, ok := pickEndpoint(req.Model)
	if !ok {
		c.JSON(http.StatusBadRequest, gin.H{"error": fmt.Sprint("找不到模型: ", req.Model)})
		return
	}
	model, err := ep.OllamaModel(actualModel)
	if err != nil {
		c.JSON(http.StatusBadRequest, gin.H{"error": fmt.Sprint("获取模型失败: ", err.Error())})
		return
	}
	c.JSON(http.StatusOK, model)
}
func getV1Models(c *gin.Context) {
	var allModels []openai.Model

	for _, ep := range config.Endpoints {
		models, err := ep.OpenAIModels()
		if err != nil {
			log.Printf("获取端点[%s]模型失败: %v", ep.ID, err)
			continue
		}
		allModels = append(allModels, models.Models...)
	}

	c.JSON(http.StatusOK, openai.ModelsList{Models: allModels})
}

func getV1Model(c *gin.Context) {
	modelId := c.Param("modelId")
	ep, actualModel, ok := pickEndpoint(modelId[1:])
	if !ok {
		c.JSON(http.StatusBadRequest, gin.H{"error": fmt.Sprintf("找不到此模型[%s]的端点", modelId)})
		return
	}
	model, err := ep.OpenAIModel(actualModel)
	if err != nil {
		c.JSON(http.StatusBadRequest, gin.H{"error": fmt.Sprintf("获取模型信息失败: %v", err)})
		return
	}
	c.JSON(http.StatusOK, model)
}
func postV1ChatCompletions(c *gin.Context) {
	var req openai.ChatCompletionRequest
	if err := c.ShouldBindJSON(&req); err != nil {
		c.JSON(http.StatusBadRequest, gin.H{"error": "解析请求体失败", "detail": err.Error()})
		return
	}
	ep, actualModel, ok := pickEndpoint(req.Model)
	if !ok {
		c.JSON(http.StatusBadRequest, gin.H{"error": fmt.Sprintf("找不到此模型[%s]的端点", req.Model)})
		return
	}

	// 修正模型 ID
	req.Model = actualModel
	resp, err := ep.NewRequest("POST", "/chat/completions", req)
	if err != nil {
		c.JSON(http.StatusBadGateway, gin.H{"error": "转发请求失败", "detail": err.Error()})
		return
	}
	defer resp.Body.Close()

	for key, values := range resp.Header {
		for _, value := range values {
			c.Writer.Header().Add(key, value)
		}
	}
	c.Status(resp.StatusCode)
	io.Copy(c.Writer, resp.Body)

}

// =============================
// 启动服务器
// =============================
func main() {
	cfg := "config.yaml"
	if len(os.Args) == 2 {
		cfg = os.Args[1]
	}
	if cfg == "help" || cfg == "-h" || cfg == "--help" {
		fmt.Printf("Usage: %s [config.yaml]", os.Args[0])
		return
	}
	if err := loadConfig(cfg); err != nil {
		log.Fatalln(err.Error())
	}
	go watchConfig(cfg)

	gin.SetMode(config.Server.Mode)

	r := gin.Default()
	r.GET("/api/version", getVersion)
	r.GET("/api/tags", getOllamaTags)
	r.POST("/api/show", postOllamaShow)

	r.GET("/models", getV1Models)
	r.GET("/v1/models", getV1Models)
	r.POST("/chat/completions", postV1ChatCompletions)
	r.POST("/v1/chat/completions", postV1ChatCompletions)

	r.GET("/models/*modelId", getV1Model)
	r.GET("/v1/models/*modelId", getV1Model)

	log.Printf("Ollama Proxy 服务启动中，监听端口 %s ...", config.Server.Listen)

	if err := r.Run(config.Server.Listen); err != nil {
		log.Fatalf("启动失败: %v", err)
	}
}
